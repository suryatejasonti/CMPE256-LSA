{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (40%) Collaboraitve Filtering Movie Recommendation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A Dictionary of movie critics and their ratings of a small set of movies\n",
    "critics={'Lisa Rose': {'Lady in the Water': 2.5, 'Snakes on a Plane': 3.5,\n",
    " 'Just My Luck': 3.0, 'Superman Returns': 3.5, 'You, Me and Dupree': 2.5,\n",
    " 'The Night Listener': 3.0},\n",
    "'Gene Seymour': {'Lady in the Water': 3.0, 'Snakes on a Plane': 3.5,\n",
    " 'Just My Luck': 1.5, 'Superman Returns': 5.0, 'The Night Listener': 3.0,\n",
    " 'You, Me and Dupree': 3.5},\n",
    "'Michael Phillips': {'Lady in the Water': 2.5, 'Snakes on a Plane': 3.0,\n",
    " 'Superman Returns': 3.5, 'The Night Listener': 4.0},\n",
    "'Claudia Puig': {'Snakes on a Plane': 3.5, 'Just My Luck': 3.0,\n",
    " 'The Night Listener': 4.5, 'Superman Returns': 4.0,\n",
    " 'You, Me and Dupree': 2.5},\n",
    "'Mick LaSalle': {'Lady in the Water': 3.0, 'Snakes on a Plane': 4.0,\n",
    " 'Just My Luck': 2.0, 'Superman Returns': 3.0, 'The Night Listener': 3.0,\n",
    " 'You, Me and Dupree': 2.0},\n",
    "'Jack Matthews': {'Lady in the Water': 3.0, 'Snakes on a Plane': 4.0,\n",
    " 'The Night Listener': 3.0, 'Superman Returns': 5.0, 'You, Me and Dupree': 3.5},\n",
    "'Toby': {'Snakes on a Plane':4.5,'You, Me and Dupree':1.0,'Superman Returns':4.0}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Suppose we are given a data set above of each customrs' rating to each movie with sacle 0-5. Stored as python dictinary format. Can you write a function with inputs of rating data and two persons, return their similarity measured by Pearson correlation. You may define and implment other similarity measure also. (10%). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Given a person, say A, could you write a function to return top n persons most similar to A (exclude A) (5%)? \n",
    "### Same dataset as above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Given a person, say A, could you write a function to recommend those unseen movies? (10%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's using another approach, can you write a function to return top n most similair movies for a given movie? (5%) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Given a movie, say B, could you write a function to recommend those peoples who have not seen this movies but they may rank this movie with high score? For example, who are those two persons who will rate \"Just My Luck\" high but they never seen this movie before? (10%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (20%) Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Data quality can be assessed in terms of accuracy, completeness, and consistency. What other factors are also important to data quality (5%)? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solutions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.  Suppose that the data for analysis includes the attribute age. The age values for the data tuples are (in increasing order) 13, 15, 16, 16, 19, 20, 20, 21, 22, 22, 25, 25, 25, 25, 30, 33, 33, 35, 35, 35, 35, 36, 40, 45, 46, 52, 70  (10%).\n",
    "\n",
    "(a) What is the mean of the data? What is the median?\n",
    "\n",
    "(b) What is the mode of the data? Comment on the data's modality (i.e., bimodal, trimodal, etc.).\n",
    "\n",
    "(c) What is the midrange of the data?\n",
    "\n",
    "(d) Can you find (roughly) the first quartile (Q1) and the third uartile (Q3) of the data? \n",
    "\n",
    "(e) Give the five-number summary, minimum, Q1, median, Q3, maximum, of the data.\n",
    "\n",
    "(f) Show a boxplot of the data from (e).\n",
    "\n",
    "(g) How is a quantile-quantile plot different from a quantile plot? (The quantile-quantile (q-q) plot is a graphical technique for determining if two data sets come from populations with a common distribution.  A q-q plot is a plot of the quantiles of the first data set against the quantiles of the second data set.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. In practical data, tuples with missing values for some attributes are a common occurrence. Describe various methods for handling this problem (5%) ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solutions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (30%) How to Acquire & Refine the Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Items, Users, Signal\n",
    "\n",
    "- **Items**: Objects to be recommended\n",
    "- **Users**: Target of the recommendation\n",
    "- **Signal**: Explicit or Implicit feedback between Items and users\n",
    "\n",
    "In our context\n",
    "- Items are **stories** posted on HN\n",
    "- Users are **users** commenting or posting stories\n",
    "- Signal are **comments** on the stories by the user signalling interest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Source of Data \n",
    "\n",
    "Lets see the sources for getting historical information posted on HackerNews\n",
    "\n",
    "- **HackerNews API** - YCombinator provides an official hackernew api through Firebase. This is a near real-time database and provides both an *items* (stories and comments) as well as an *users* api.  It is available at https://github.com/HackerNews/API\n",
    "\n",
    "- **BigQuery** : Google Big Query has a daily updated HackerNews public dataset available (from 2006 to date). It only has *items* information. It is available at https://bigquery.cloud.google.com/table/bigquery-public-data:hacker_news.full"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Items (Stories) \n",
    "\n",
    "The schema for the Big Query table is\n",
    "\n",
    "| Field       | Type      | Description                           |\n",
    "|:------------|----------:|:--------------------------------------|\n",
    "| by          | STRING    | The username of the item's author.    |\n",
    "| score       | INTEGER   | Story score                           |\n",
    "| time        | INTEGER   | Unix time                             |\n",
    "| timestamp   | TIMESTAMP | Timestamp for the unix time           |\n",
    "| title       | STRING    | Story title                           |\n",
    "| type        | STRING    | Type of details (comment, story, ...) |\n",
    "| url         | STRING    | Story url                             |\n",
    "| text        | STRING    | Story or comment text                 |\n",
    "| parent      | INTEGER   | Parent comment ID                     |\n",
    "| deleted     | BOOLEAN   | Is deleted?                           |\n",
    "| dead        | BOOLEAN   | Is dead?                              |\n",
    "| descendants | INTEGER   | Number of story or poll descendants   |\n",
    "| id          | INTEGER   | The item's unique id.                 |\n",
    "| ranking     | INTEGER   | Comment ranking                       |\n",
    "\n",
    "\n",
    "- Get all the stories posted on HackerNews in **2017 (till date)**. \n",
    "- To ensure a relevant set, we will limit the stories which have atleast have **score of 5 points or more** on them. \n",
    "\n",
    "This dataset is available in `stories2017score5.gzip`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Could you read file stories2017score5.csv.gzip by Python pandas package and plot histogram of scores(10%) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most Popular Stories (5%)\n",
    "\n",
    "For this exercise:\n",
    "- Let us choose the most popular stories with a score > 500 over the last year\n",
    "- Lets keep the columns - user (by), userId (id), score (score), title (title)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Item (Story) - User - Signal (Comment) Lets get all the comments from the database by reviewing fifle (bycomments2017score5.csv.gz) and keep only the user and story information. Please see what problems in data and dealing with them. Show me processed data shapes (how many rows and column) and first five rows of  processed data (5%).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Users information (10%). \n",
    "\n",
    "We need to get the details for all the users that have made comments in our list of stories.\n",
    "\n",
    "Also, we are interested in their details\n",
    "- How long have they been on the HN platform? (Created)\n",
    "- How active they are? (# of comments)?\n",
    "- How good they are to the HN community (karma)?\n",
    "\n",
    "We will need to get the User Information from HN\n",
    "\n",
    "- **Hacker News API**: Official API hosted on Firebase - https://github.com/HackerNews/API\n",
    "- **Haxor**: Unofficial HN Python API c- https://github.com/avinassh/haxor\n",
    "- **Ascynio-HN**: A very fast async Python API for HackerNews - https://github.com/itielshwartz/asyncio-hn\n",
    "\n",
    "### Think how you use API to get all uses data information. Download them and save them. Will be used later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (30%) Feature Engineering (Data Transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do we create the user-item matrix for the HackerNews Dataset\n",
    "- OneHot Encoding\n",
    "- Sparse Matrix (for scalability)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is OneHot Encoding and its purpose (5%). Could you apply OneHot to encode previous obtained story user comment data from bycomments2017score5.csv.gz ? Show encoded data previous few rows. (10%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solutions:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Could you what is Coordinate list (COO) compression format for sparse matrix generation (5%)\n",
    "### Implment this to compress previous obtained story user comment data from bycomments2017score5.csv.gz (10%)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solutions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
